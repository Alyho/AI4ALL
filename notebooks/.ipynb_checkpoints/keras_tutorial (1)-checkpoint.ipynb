{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Keras Tutorial\n",
    "\n",
    "Keras is a high-level neural networks API, written in Python and capable of running on top of certain lower-level frameworks like Tensorflow. It is one level above Tensorflow and aims at implementing the Deep Learning pipelines easily and quickly.\n",
    "\n",
    "For our final project, we will be using Keras to build the entire pipeline from scratch. This includes data pre-processing, feature extraction (with CNN) and classification (with fully-connected nets). The goal of this tutorial is to give you adequate knowledge to prepare you for the final project.\n",
    "\n",
    "What is good about Keras compared with other Deep Learning frameworks:\n",
    "* It's high-level, which means that you can implement complex things with several lines of simple code\n",
    "* It works directly with NumPy arrays, so you don't have to spend extra time on creating a Python class for dataset like PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember in the class we talked about the pipeline of a real computer vision system, in which we:\n",
    "\n",
    "1. First clean the data to the format to be used for later steps (which includes data loading, data pre-processing, dataset splitting (we'll talk about this on Friday), data augmentation (which we're not gonna cover), etc);\n",
    "\n",
    "2. Then we build the model for feature extraction as well as for final regression / classification. Remember we have many choices like linear model, fully connected neural nets, convolutional neural nets, etc. And we can implement these models very easily in Keras with just one line of code;\n",
    "\n",
    "3. After we get the data and the model, we need to code up the optimization part (for which we'll use gradient descent). \n",
    "\n",
    "In this tutorial, we'll go over these parts sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Pre-processing\n",
    "\n",
    "So in Keras we don't need anything specific for data, we just use NumPy and represent our data in Numpy arrays. Now we're gonna create some fake data to be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# Create random numpy arrays (ldata loading)\n",
    "rand_data = np.random.random((1000, 32, 32, 3)) # We have 1000 fake images with spatial size 32 * 32\n",
    "rand_label = np.array([0]*500 + [1]*500)        # Create fake binary labels for these images  \n",
    "\n",
    "print(rand_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 32, 32, 3)\n",
      "(50, 32, 32, 3)\n",
      "(50, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# Split data into train, validation and test sets (we'll talk more about this on Friday)\n",
    "train_ratio, val_ratio = 0.9, 0.05\n",
    "\n",
    "X_train = rand_data[:int(rand_data.shape[0]*train_ratio), ...] # ... means all the other axes\n",
    "y_train = rand_label[:int(rand_data.shape[0]*train_ratio), ...]\n",
    "\n",
    "X_val = rand_data[int(rand_data.shape[0]*train_ratio):int(rand_data.shape[0]*(train_ratio+val_ratio)), ...]\n",
    "y_val = rand_label[int(rand_data.shape[0]*train_ratio):int(rand_data.shape[0]*(train_ratio+val_ratio)), ...]\n",
    "\n",
    "X_test = rand_data[int(rand_data.shape[0]*(train_ratio+val_ratio)):, ...]\n",
    "y_test = rand_label[int(rand_data.shape[0]*(train_ratio+val_ratio)):, ...]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model construction\n",
    "\n",
    "Now we have all the data, next we're gonna build our model for feature extraction as well as classification. In Keras, you can easily build many models, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0705 09:28:21.954539 140737189909440 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential # Sequential is one of the main models in Keras, which is basically a sequentially stacked series of layers\n",
    "\n",
    "model = Sequential() # Initialize a Sequential model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0705 09:28:22.000301 140737189909440 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0705 09:28:22.012723 140737189909440 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First we'll use fully-connected neural nets\n",
    "from keras.layers import Dense # Dense is Keras's name for fully connected layers\n",
    "\n",
    "# We can stack layers like lego blocks by simplying using `add()`\n",
    "# `units` is the number of neurons\n",
    "# `activation` is the nonlinear function we add for each layer\n",
    "# We only need to specify `input_dim` which is the input dimension for the layer for the input layer, because for later layers the input is just the output from last layer\n",
    "# Once again, the number of neurons in hidden layers (e.g., 64 and 16 here) are design choices\n",
    "\n",
    "model.add(Dense(units=64, activation='sigmoid', input_dim=32*32*3)) \n",
    "model.add(Dense(units=16, activation='sigmoid'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0705 09:28:22.084432 140737189909440 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0705 09:28:22.115885 140737189909440 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0705 09:28:22.128931 140737189909440 deprecation.py:323] From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Once the model is build, we then configure the learning process with `compile()`\n",
    "# We need to specify the loss function, the optimizer and the metric we use to evaluate our model\n",
    "# For loss here we're using a function called binary cross-entropy loss, which is specifically for binary classification\n",
    "# For optimizer we're using gradient descent, which is written as 'sgd' in Keras\n",
    "# Since we're doing classification, normally the classification accuracy is how we evaluate the model\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above is actually a convenient way that Keras provides for easy implementation. If you want to have more control over the learning process (e.g., the learning rate), you can use the following:\n",
    "\n",
    "model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.SGD(lr=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 3072)\n",
      "(50, 3072)\n",
      "(50, 3072)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0705 09:28:22.530036 140737189909440 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 900 samples, validate on 50 samples\n",
      "Epoch 1/5\n",
      "900/900 [==============================] - 1s 648us/step - loss: 0.6874 - val_loss: 0.7875\n",
      "Epoch 2/5\n",
      "900/900 [==============================] - 0s 106us/step - loss: 0.6873 - val_loss: 0.7908\n",
      "Epoch 3/5\n",
      "900/900 [==============================] - 0s 114us/step - loss: 0.6873 - val_loss: 0.7884\n",
      "Epoch 4/5\n",
      "900/900 [==============================] - 0s 165us/step - loss: 0.6873 - val_loss: 0.7917\n",
      "Epoch 5/5\n",
      "900/900 [==============================] - 0s 175us/step - loss: 0.6872 - val_loss: 0.7910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb3157a6a0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Up to this point we're all doing configurations. Now everything is set up so we're letting the model do real things!\n",
    "\n",
    "# Since now we're using a fully-connected nets, remember we need to flatten the image to a single long vector first\n",
    "X_train_flat = X_train.reshape((-1, 32*32*3)) # -1 means letting NumPy to figure this axis out automatically\n",
    "X_val_flat = X_val.reshape((-1, 32*32*3))\n",
    "X_test_flat = X_test.reshape((-1, 32*32*3))\n",
    "\n",
    "print(X_train_flat.shape)\n",
    "print(X_val_flat.shape)\n",
    "print(X_test_flat.shape)\n",
    "\n",
    "# Then use fit() to actually train our model\n",
    "# epochs is basically how many iterations we want for the update process. The model needs some time to reach the optimal state!\n",
    "# batch_size is how many images we use each time to estimate the gradient. Remember that the more we use the more accurate each update will be, but it will also be slower\n",
    "\n",
    "model.fit(X_train_flat, y_train, epochs=5, batch_size=32, validation_data=(X_val_flat, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 146us/step\n",
      "The test accuracy is: 0.7947416734695435\n",
      "The predicted probabilities are: [[0.45477158]\n",
      " [0.44668323]\n",
      " [0.46021828]\n",
      " [0.4545601 ]\n",
      " [0.46038583]\n",
      " [0.44986328]\n",
      " [0.45142794]\n",
      " [0.44875067]\n",
      " [0.4542802 ]\n",
      " [0.45209658]\n",
      " [0.45280486]\n",
      " [0.44890785]\n",
      " [0.44748646]\n",
      " [0.44378874]\n",
      " [0.44569457]\n",
      " [0.45053044]\n",
      " [0.43157098]\n",
      " [0.45155236]\n",
      " [0.44229943]\n",
      " [0.4472689 ]\n",
      " [0.44170603]\n",
      " [0.44938493]\n",
      " [0.4615542 ]\n",
      " [0.4534109 ]\n",
      " [0.4528866 ]\n",
      " [0.45232597]\n",
      " [0.45234382]\n",
      " [0.45675147]\n",
      " [0.44662973]\n",
      " [0.45660383]\n",
      " [0.45195985]\n",
      " [0.4648028 ]\n",
      " [0.4373597 ]\n",
      " [0.44333586]\n",
      " [0.45518896]\n",
      " [0.46386677]\n",
      " [0.44823638]\n",
      " [0.45028704]\n",
      " [0.4437078 ]\n",
      " [0.4567679 ]\n",
      " [0.46571988]\n",
      " [0.4489488 ]\n",
      " [0.46084955]\n",
      " [0.46225977]\n",
      " [0.44155896]\n",
      " [0.4604025 ]\n",
      " [0.45427603]\n",
      " [0.46301183]\n",
      " [0.45400363]\n",
      " [0.44274256]]\n",
      "The predicted class labels are: [[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "# Now let's see how our model does\n",
    "acc = model.evaluate(X_test_flat, y_test)\n",
    "print('The test accuracy is: {}'.format(acc))\n",
    "\n",
    "# And make predictions\n",
    "prob = model.predict(X_test_flat) # These are probabilities, and we want to convert them to class labels\n",
    "label = np.array(prob > 0.5, dtype=int)\n",
    "\n",
    "print('The predicted probabilities are: {}'.format(prob))\n",
    "print('The predicted class labels are: {}'.format(label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0705 09:38:03.568291 140737189909440 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 900 samples, validate on 50 samples\n",
      "Epoch 1/10\n",
      "900/900 [==============================] - 1s 639us/step - loss: 0.9556 - val_loss: 0.4808\n",
      "Epoch 2/10\n",
      "900/900 [==============================] - 0s 230us/step - loss: 0.7103 - val_loss: 0.7288\n",
      "Epoch 3/10\n",
      "900/900 [==============================] - 0s 229us/step - loss: 0.6886 - val_loss: 0.8122\n",
      "Epoch 4/10\n",
      "900/900 [==============================] - 0s 231us/step - loss: 0.6872 - val_loss: 0.8191\n",
      "Epoch 5/10\n",
      "900/900 [==============================] - 0s 250us/step - loss: 0.6873 - val_loss: 0.7729\n",
      "Epoch 6/10\n",
      "900/900 [==============================] - 0s 270us/step - loss: 0.6876 - val_loss: 0.7901\n",
      "Epoch 7/10\n",
      "900/900 [==============================] - 0s 262us/step - loss: 0.6874 - val_loss: 0.8158\n",
      "Epoch 8/10\n",
      "900/900 [==============================] - 0s 233us/step - loss: 0.6873 - val_loss: 0.7891\n",
      "Epoch 9/10\n",
      "900/900 [==============================] - 0s 231us/step - loss: 0.6873 - val_loss: 0.8059\n",
      "Epoch 10/10\n",
      "900/900 [==============================] - 0s 232us/step - loss: 0.6877 - val_loss: 0.8294\n",
      "50/50 [==============================] - 0s 140us/step\n",
      "The test accuracy is: 0.8300326800346375\n",
      "The predicted probabilities are: [[0.4384126 ]\n",
      " [0.43626654]\n",
      " [0.43764737]\n",
      " [0.44211346]\n",
      " [0.437671  ]\n",
      " [0.4353158 ]\n",
      " [0.42938676]\n",
      " [0.43507063]\n",
      " [0.43988162]\n",
      " [0.4348964 ]\n",
      " [0.44077662]\n",
      " [0.43446577]\n",
      " [0.4372483 ]\n",
      " [0.43768936]\n",
      " [0.43128538]\n",
      " [0.4339156 ]\n",
      " [0.43521297]\n",
      " [0.435493  ]\n",
      " [0.43735772]\n",
      " [0.43590128]\n",
      " [0.43875676]\n",
      " [0.43364885]\n",
      " [0.43834388]\n",
      " [0.43801144]\n",
      " [0.43543148]\n",
      " [0.4406451 ]\n",
      " [0.43926546]\n",
      " [0.43634355]\n",
      " [0.43319237]\n",
      " [0.4338613 ]\n",
      " [0.44394666]\n",
      " [0.4362685 ]\n",
      " [0.43786955]\n",
      " [0.43797907]\n",
      " [0.439364  ]\n",
      " [0.43737888]\n",
      " [0.43387103]\n",
      " [0.42664778]\n",
      " [0.43611446]\n",
      " [0.4285552 ]\n",
      " [0.4279863 ]\n",
      " [0.43221375]\n",
      " [0.43180525]\n",
      " [0.43781084]\n",
      " [0.43524134]\n",
      " [0.4375616 ]\n",
      " [0.43545437]\n",
      " [0.43525988]\n",
      " [0.44066086]\n",
      " [0.43693772]]\n",
      "The predicted class labels are: [[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "# As we can expect, the results are totally random\n",
    "# You can also play with other models, e.g., convnets\n",
    "# So we do the same procedure once more\n",
    "\n",
    "model = Sequential() # Re-initialize the model\n",
    "\n",
    "# Feature extractor\n",
    "# We're using such an architecture: conv -> maxpool -> conv -> maxpool\n",
    "# 'same' padding means we zero-pad the images so that the output will be of the same size as the input\n",
    "model.add(keras.layers.Conv2D(filters=16, kernel_size=3, strides=(2, 2), padding='same'))\n",
    "model.add(keras.layers.Activation('sigmoid'))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2))) # By default the stride is the same as the pooling size\n",
    "\n",
    "model.add(keras.layers.Conv2D(filters=32, kernel_size=2, strides=(1, 1), padding='same'))\n",
    "model.add(keras.layers.Activation('relu')) # ReLU is another kind of non-linear function\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Classifier\n",
    "# We're using a 2-layer FC net for classification \n",
    "model.add(keras.layers.Flatten())\n",
    "\n",
    "model.add(keras.layers.Dense(32))\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "\n",
    "model.add(keras.layers.Dense(1))\n",
    "model.add(keras.layers.Activation('sigmoid'))\n",
    "\n",
    "# Compilation\n",
    "model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.SGD(lr=0.001))\n",
    "\n",
    "# Training\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluation\n",
    "acc = model.evaluate(X_test, y_test)\n",
    "print('The test accuracy is: {}'.format(acc))\n",
    "\n",
    "# And make predictions\n",
    "prob = model.predict(X_test) # These are probabilities, and we want to convert them to class labels\n",
    "label = np.array(prob > 0.5, dtype=int)\n",
    "\n",
    "print('The predicted probabilities are: {}'.format(prob))\n",
    "print('The predicted class labels are: {}'.format(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
